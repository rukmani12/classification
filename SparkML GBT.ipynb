{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "import ibmos2spark\n\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'api_key': 'iTfSE_YVE6zqnxjd1oK0E37R2aVYY4dFhXFFCljE1AcJ',\n    'service_id': 'iam-ServiceId-d4b06e46-293a-4417-b76c-2f16076a9353',\n    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token'}\n\nconfiguration_name = 'os_b0f1407510994fd1b793b85137baafb8_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n\n\ndf = spark.read.parquet(cos.url('hmp.parquet', 'courseraml-donotdelete-pr-qve0ttzezdeodc'))", 
            "cell_type": "code", 
            "execution_count": 3, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "df.createOrReplaceTempView('df')\ndf_two_class = spark.sql(\"select * from df where class in ('Use_telephone','Standup_chair')\")", 
            "cell_type": "code", 
            "execution_count": 4, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "splits = df_two_class.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", 
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)", 
            "cell_type": "code", 
            "execution_count": 10, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.classification import GBTClassifier\n\ngbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)", 
            "cell_type": "code", 
            "execution_count": 11, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,gbt])", 
            "cell_type": "code", 
            "execution_count": 14, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "model = pipeline.fit(df_train)", 
            "cell_type": "code", 
            "execution_count": 13, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "prediction = model.transform(df_train)", 
            "cell_type": "code", 
            "execution_count": 15, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n    \nbinEval.evaluate(prediction) ", 
            "cell_type": "code", 
            "execution_count": 16, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.912288266091777"
                    }, 
                    "execution_count": 16
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "prediction = model.transform(df_test)", 
            "cell_type": "code", 
            "execution_count": 17, 
            "outputs": [], 
            "metadata": {}
        }, 
        {
            "source": "binEval.evaluate(prediction) ", 
            "cell_type": "code", 
            "execution_count": 19, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9024718551150269"
                    }, 
                    "execution_count": 19
                }
            ], 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark", 
            "name": "python36", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }
}
